{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Exercise 1: Visualizing Decision Boundaries\n",
        "Goal: Show how individual trees and the full random forest behave."
      ],
      "metadata": {
        "id": "hOyKhLXZ-djc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the Scikit-Learn lib (SKLearn), it is built in to Colab already so no need to pip install.\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.inspection import DecisionBoundaryDisplay"
      ],
      "metadata": {
        "id": "uO_MUi5f-jJZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create synthetic dataset\n",
        "X, y = make_classification(n_features=2, n_informative=2, n_redundant=0,\n",
        "                           n_clusters_per_class=1, n_samples=500, random_state=10)"
      ],
      "metadata": {
        "id": "LRP9J20E-xow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train a single decision tree\n",
        "tree = DecisionTreeClassifier(max_depth=3).fit(X, y)"
      ],
      "metadata": {
        "id": "vz1dKE2G-yrR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train a random forest\n",
        "forest = RandomForestClassifier(n_estimators=100, max_depth=3).fit(X, y)"
      ],
      "metadata": {
        "id": "rb8AE6eJ_BEY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot decision boundaries\n",
        "_, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
        "DecisionBoundaryDisplay.from_estimator(tree, X, response_method='predict', ax=ax[0], cmap=\"coolwarm\")\n",
        "ax[0].scatter(X[:, 0], X[:, 1], c=y, edgecolor='k')\n",
        "ax[0].set_title(\"Single Tree\")\n",
        "\n",
        "DecisionBoundaryDisplay.from_estimator(forest, X, response_method='predict', ax=ax[1], cmap=\"coolwarm\")\n",
        "ax[1].scatter(X[:, 0], X[:, 1], c=y, edgecolor='k')\n",
        "ax[1].set_title(\"Random Forest\")"
      ],
      "metadata": {
        "id": "9gx7sjCk_IHI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task**\n",
        "\n",
        "Change max_depth, n_estimators and add noise features to see the impact of these changes."
      ],
      "metadata": {
        "id": "o6IIY5S6_L1z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Questions**\n",
        "\n",
        "Pair up and answer the following questions together:\n",
        "*   How 'good' is the intial classification? What are the fail states?\n",
        "*   What is the optimum setting for both approaches?\n",
        "\n",
        "\n",
        "________________________________________________________________________________"
      ],
      "metadata": {
        "id": "CYSSWNOo_rUo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " # Exercise 2: Tree vs Forest Accuracy with Real Data\n",
        " Goal: Show accuracy difference and robustness of Random Forests."
      ],
      "metadata": {
        "id": "66tIlajGAgkW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_wine\n",
        "\n",
        "# Load real data\n",
        "X, y = load_wine(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
        "\n",
        "tree = DecisionTreeClassifier().fit(X_train, y_train)\n",
        "forest = RandomForestClassifier(n_estimators=100).fit(X_train, y_train)\n",
        "\n",
        "print(\"Tree accuracy:\", accuracy_score(y_test, tree.predict(X_test)))\n",
        "print(\"Forest accuracy:\", accuracy_score(y_test, forest.predict(X_test)))"
      ],
      "metadata": {
        "id": "AFhv7TO3AgAW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task**\n",
        "\n",
        "Compare performance across datasets (load_iris, load_digits, etc.):\n",
        "\n",
        "*   https://scikit-learn.org/stable/datasets/toy_dataset.html\n",
        "\n",
        "Test the addition and removal of features and check the sensitivity of our classifiers to this.\n"
      ],
      "metadata": {
        "id": "4OsUKBQ1Amfr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Questions**\n",
        "\n",
        "Pair up and answer the following question together:\n",
        "*   What is the average perforamnce gain (expressed as accuracy) differential between the tree and the forest for the wine and the iris datasets?\n",
        "\n",
        "________________________________________________________________________________"
      ],
      "metadata": {
        "id": "3Fb8sUg8BVHi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Exercise 3: Apply to Remote Sensing-Like Data\n",
        "Goal: Use a simplified \"satellite\" dataset with spectral bands and labels."
      ],
      "metadata": {
        "id": "J9f5muDOAJtg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Simulated remote sensing dataset\n",
        "np.random.seed(42)\n",
        "n_samples = 500\n",
        "bands = ['blue', 'green', 'red', 'nir']\n",
        "X = np.random.rand(n_samples, len(bands))\n",
        "y = (X[:, 2] > 0.5).astype(int)  # Let's say 'vegetation (1) vs not (0)' based on red\n",
        "\n",
        "rf = RandomForestClassifier().fit(X, y)\n",
        "print(\"Accuracy:\", accuracy_score(y, rf.predict(X)))\n",
        "\n",
        "importances = rf.feature_importances_\n",
        "pd.Series(importances, index=bands).plot(kind='bar', title='Band Importance')\n"
      ],
      "metadata": {
        "id": "lJlFCLaBAPnj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "\n",
        "# Combine into DataFrame for inspection\n",
        "df = pd.DataFrame(X, columns=bands)\n",
        "df['class'] = y\n",
        "\n",
        "# Visualize the distributions and verify that 'red' is our spectrally significantly different band\n",
        "sns.pairplot(df, hue='class', plot_kws={'alpha': 0.6, 's': 20})\n",
        "plt.suptitle(\"Synthetic Spectral Data by Class\", y=1.02)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0XXY5xsfCveD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simulated Pseudo-Image from Synthetic Data\n",
        "\n",
        "# Assume the synthetic data: X shape (n_samples, 4), bands = ['blue', 'green', 'red', 'nir']\n",
        "# Let's reshape it into a 2D grid\n",
        "n_rows, n_cols = 25, 20  # 500 samples -> 25 x 20 grid\n",
        "assert n_rows * n_cols == X.shape[0]\n",
        "\n",
        "# Reshape each band\n",
        "blue = X[:, 0].reshape(n_rows, n_cols)\n",
        "green = X[:, 1].reshape(n_rows, n_cols)\n",
        "red = X[:, 2].reshape(n_rows, n_cols)\n",
        "\n",
        "# Stack into an RGB image\n",
        "rgb = np.stack([red, green, blue], axis=-1)\n",
        "\n",
        "# Optional: stretch to [0,1] for display (though values are already 0â€“1 in synthetic data)\n",
        "rgb = np.clip(rgb, 0, 1)\n",
        "\n",
        "# Display\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.imshow(rgb)\n",
        "plt.title(\"Simulated Remote Sensing RGB Composite\")\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "lrkEBJk4ZjgI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task**\n",
        "\n",
        "Add 'fake' noise bands and observe how the model performance changes. Then adjust the logic of how labels are generated and check classifier limits."
      ],
      "metadata": {
        "id": "idO6Y8CZCO3V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question**\n",
        "\n",
        "What happens to the accuracy when we add a band that deliberately gives the 'opposite' signal to the 'red' band that we set up in our first run of this?\n",
        "\n",
        "________________________________________________________________________________"
      ],
      "metadata": {
        "id": "T4HnbYf9DKYB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 4: Putting It All Together with SVM\n",
        "\n",
        "Goal: Apply classification, feature importance intuition, and decision boundary visualization using an SVM instead of a Random Forest."
      ],
      "metadata": {
        "id": "HGnFuASSYnb-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.inspection import DecisionBoundaryDisplay\n",
        "\n",
        "# Use same synthetic data from Exercise 3\n",
        "X = df[bands].values\n",
        "y = df['class'].values\n",
        "\n",
        "# Create an SVM pipeline with feature scaling\n",
        "svm = make_pipeline(StandardScaler(), SVC(kernel='rbf', C=1.0, gamma='scale'))\n",
        "svm.fit(X, y)\n",
        "\n",
        "# Evaluate performance\n",
        "print(\"SVM Accuracy:\", accuracy_score(y, svm.predict(X)))\n",
        "\n",
        "# Try cross-validation\n",
        "cv_score = cross_val_score(svm, X, y, cv=5).mean()\n",
        "print(\"Cross-validated accuracy:\", round(cv_score, 3))\n"
      ],
      "metadata": {
        "id": "NfUuTp9dYtCG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tasks**\n",
        "*   Try different kernels: 'linear', 'poly', 'rbf', 'sigmoid'.\n",
        "*   Adjust C and gamma to observe under/overfitting.\n",
        "*   Compare how SVM performs vs. Random Forest on the same data.\n",
        "*   Replace synthetic data with a real-world dataset (e.g. load_iris())"
      ],
      "metadata": {
        "id": "rLPjdgSTY2dr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question**\n",
        "What is the most accurate kernel and C value to use for your synthetic remote sensing dataset?\n"
      ],
      "metadata": {
        "id": "d0wlNiMDZQwR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optional Visualization: Project to 2D with PCA and Plot Decision Boundary"
      ],
      "metadata": {
        "id": "fM6XZ_ujZHQZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Project to 2D using PCA for visualization\n",
        "pca = PCA(n_components=2)\n",
        "X_2D = pca.fit_transform(X)\n",
        "\n",
        "# Train SVM on 2D data\n",
        "svm_2d = make_pipeline(StandardScaler(), SVC(kernel='rbf', C=1.0, gamma='scale'))\n",
        "svm_2d.fit(X_2D, y)\n",
        "\n",
        "# Plot decision boundary\n",
        "DecisionBoundaryDisplay.from_estimator(svm_2d, X_2D, response_method=\"predict\", cmap=\"coolwarm\")\n",
        "plt.scatter(X_2D[:, 0], X_2D[:, 1], c=y, edgecolor='k', cmap=\"coolwarm\", alpha=0.6)\n",
        "plt.title(\"SVM Decision Boundary (PCA-Reduced Data)\")\n",
        "plt.xlabel(\"PC1\")\n",
        "plt.ylabel(\"PC2\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "a4S9vcuNY08r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optional Structured Synthetic Data Generation for Remote Sensing Classification\n",
        "\n",
        "Try taking this alternative synethetic data generation code and inserting back up the notebook where you generate your test 'remote sensing' data. What difference does this make to how the methods perform?"
      ],
      "metadata": {
        "id": "13rwI0rJZ3pX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Set grid dimensions\n",
        "n_rows, n_cols = 25, 20\n",
        "n_samples = n_rows * n_cols\n",
        "bands = ['blue', 'green', 'red', 'nir']\n",
        "\n",
        "# Create coordinate grid\n",
        "xx, yy = np.meshgrid(np.arange(n_cols), np.arange(n_rows))\n",
        "x_coords = xx.flatten()\n",
        "y_coords = yy.flatten()\n",
        "\n",
        "# Simulate structured class label:\n",
        "# Vegetation occurs in a central elliptical region\n",
        "x_center, y_center = n_cols / 2, n_rows / 2\n",
        "ellipse_mask = ((x_coords - x_center)**2 / (0.3 * n_cols)**2 + (y_coords - y_center)**2 / (0.4 * n_rows)**2) < 1\n",
        "y = ellipse_mask.astype(int)  # 1 = vegetation, 0 = other\n",
        "\n",
        "# Generate band values\n",
        "X = np.zeros((n_samples, 4))\n",
        "\n",
        "# Non-vegetation class (class 0): lower NIR, higher red\n",
        "X[y == 0, 0] = np.random.normal(0.3, 0.1, size=(y == 0).sum())  # blue\n",
        "X[y == 0, 1] = np.random.normal(0.4, 0.1, size=(y == 0).sum())  # green\n",
        "X[y == 0, 2] = np.random.normal(0.6, 0.1, size=(y == 0).sum())  # red\n",
        "X[y == 0, 3] = np.random.normal(0.2, 0.05, size=(y == 0).sum()) # nir\n",
        "\n",
        "# Vegetation class (class 1): lower red, higher NIR\n",
        "X[y == 1, 0] = np.random.normal(0.2, 0.05, size=(y == 1).sum()) # blue\n",
        "X[y == 1, 1] = np.random.normal(0.4, 0.1, size=(y == 1).sum())  # green\n",
        "X[y == 1, 2] = np.random.normal(0.3, 0.05, size=(y == 1).sum()) # red\n",
        "X[y == 1, 3] = np.random.normal(0.7, 0.1, size=(y == 1).sum())  # nir\n",
        "\n",
        "# Clip all values to [0, 1]\n",
        "X = np.clip(X, 0, 1)\n",
        "\n",
        "# Create DataFrame for exploration\n",
        "df = pd.DataFrame(X, columns=bands)\n",
        "df['class'] = y\n",
        "df['x'] = x_coords\n",
        "df['y'] = y_coords\n"
      ],
      "metadata": {
        "id": "lAiOqsyDZ8vd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}